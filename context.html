<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Place Recognition and Context Awareness | Indoor Navigation Tutorial</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <nav class="sidebar">
        <div class="nav-header">
            <h2>Navigation</h2>
        </div>
        <ul class="nav-links">
            <li><a href="index.html">Introduction</a></li>
            <li><a href="sensors.html">Sensor Possibilities</a></li>
            <li><a href="techniques.html">Detection Techniques</a></li>
            <li><a href="context.html" class="active">Place Recognition</a></li>
            <li><a href="review.html">Successes & Failures</a></li>
            <li><a href="future.html">Challenges & Future</a></li>
            <li><a href="bibliography.html">Bibliography</a></li>
        </ul>
    </nav>

    <main class="content">
        <header class="page-header">
            <h1>Place Recognition and Context Awareness</h1>
            <p class="subtitle">Building Spatial Memory for Indoor Navigation</p>
        </header>

        <!-- AUDIO PLACEHOLDER: Add a voice track here summarizing place recognition concepts -->
        <div class="audio-container">
            <audio controls>
                <source src="audio/page4-context.mp3" type="audio/mpeg">
                Your browser does not support the audio element.
            </audio>
            <p class="audio-caption">üîä Audio Summary (2:30)</p>
        </div>

        <section class="context-section">
            <h2>Beyond Landmark Detection: Spatial Understanding</h2>
            <p>
                While the detection techniques discussed in the previous section enable identification of individual landmarks, effective navigation requires a higher level of spatial understanding: <strong>where am I?</strong> and <strong>how do I get to my destination?</strong> This section explores Visual Place Recognition (VPR) and landmark graph construction‚Äîtechniques that enable navigation systems to build spatial memory, recognize previously visited locations, and provide turn-by-turn guidance within indoor environments.
            </p>

            <div class="vpr-analysis">
                <h3>Visual Place Recognition (VPR): Recognizing Familiar Locations</h3>
                
                <h4>Problem Definition</h4>
                <p>
                    Visual Place Recognition addresses the question: "Have I been here before?" Given a current camera image, VPR systems search a database of previously observed locations to find matches, enabling:
                </p>
                <ul>
                    <li><strong>Localization:</strong> Determining current position by matching to known locations</li>
                    <li><strong>Loop closure:</strong> Recognizing return to previously visited places to correct accumulated positioning errors</li>
                    <li><strong>Relocalization:</strong> Recovering position after tracking loss or system restart</li>
                    <li><strong>Familiar route guidance:</strong> Providing navigation instructions based on recognized landmarks from previous visits</li>
                </ul>

                <h4>The Challenge of Appearance Variation</h4>
                <p>
                    VPR must handle significant appearance changes between visits to the same location:
                </p>
                <ul>
                    <li><strong>Viewpoint variation:</strong> Approaching from different directions or angles</li>
                    <li><strong>Lighting changes:</strong> Different times of day, artificial lighting variations</li>
                    <li><strong>Dynamic elements:</strong> People, furniture rearrangement, opened/closed doors</li>
                    <li><strong>Temporal changes:</strong> Renovations, decorations, seasonal modifications</li>
                </ul>
                <p>
                    Effective VPR requires image representations that are <strong>invariant</strong> to these variations while remaining <strong>discriminative</strong> enough to distinguish different locations.
                </p>

                <h4>Classical Approach: Bag-of-Visual-Words</h4>
                <p>
                    Traditional VPR systems used hand-crafted features:
                </p>
                <ol>
                    <li><strong>Feature extraction:</strong> Detect keypoints (SIFT, SURF) and compute descriptors</li>
                    <li><strong>Visual vocabulary:</strong> Cluster descriptors from many images to create a "codebook" of visual words</li>
                    <li><strong>Image representation:</strong> Represent each image as a histogram of visual word occurrences</li>
                    <li><strong>Matching:</strong> Compare histograms using similarity metrics (cosine similarity, œá¬≤ distance)</li>
                </ol>
                <p>
                    <strong>Limitations:</strong> Sensitive to viewpoint changes, computationally expensive for large databases, and limited discriminative power.
                </p>

                <h4>Modern Approach: Deep Learning-Based VPR</h4>
                <p>
                    Contemporary VPR systems leverage deep convolutional neural networks to learn robust image representations directly from data <a href="bibliography.html#ref5" class="citation">[5]</a>.
                </p>

                <h5>NetVLAD: Learnable VLAD Pooling</h5>
                <p>
                    NetVLAD (Vector of Locally Aggregated Descriptors) represents the state-of-the-art in learned VPR representations. The architecture consists of:
                </p>
                <ol>
                    <li><strong>CNN backbone:</strong> Extracts dense feature maps from input image (e.g., VGG, ResNet)</li>
                    <li><strong>NetVLAD layer:</strong> Aggregates local features into a fixed-size global descriptor</li>
                    <li><strong>Normalization:</strong> L2 normalization for robust similarity comparison</li>
                </ol>

                <div class="algorithm-box">
                    <h4>Algorithm 2: NetVLAD-Based Visual Place Recognition</h4>
                    <pre>
INPUT: Query image I_query, Database of reference images {I_1, I_2, ..., I_N}
OUTPUT: Most similar location match and confidence score

OFFLINE PHASE (Database Construction):
1. FOR each reference image I_i in database:
       // Extract CNN features
       features_i = CNN_backbone(I_i)  // Shape: H√óW√óD
       
       // Apply NetVLAD aggregation
       descriptor_i = NetVLAD(features_i)  // Shape: K√óD
       
       // Flatten and normalize
       descriptor_i = flatten(descriptor_i)  // Shape: K*D
       descriptor_i = L2_normalize(descriptor_i)
       
       // Store in database
       database.add(descriptor_i, location_i)

ONLINE PHASE (Query Matching):
2. // Extract descriptor for query image
   features_query = CNN_backbone(I_query)
   descriptor_query = NetVLAD(features_query)
   descriptor_query = flatten(descriptor_query)
   descriptor_query = L2_normalize(descriptor_query)

3. // Compute similarity to all database images
   similarities = []
   FOR each descriptor_i in database:
       // Cosine similarity (dot product of normalized vectors)
       sim = dot_product(descriptor_query, descriptor_i)
       similarities.add((sim, location_i))

4. // Find best match
   SORT similarities by sim (descending)
   best_match = similarities[0]
   
5. // Confidence assessment
   IF best_match.sim > threshold (e.g., 0.7):
       // Check if significantly better than second-best
       IF best_match.sim - similarities[1].sim > margin (e.g., 0.1):
           RETURN (best_match.location, HIGH_CONFIDENCE)
       ELSE:
           RETURN (best_match.location, MEDIUM_CONFIDENCE)
   ELSE:
       RETURN (NO_MATCH, LOW_CONFIDENCE)

NetVLAD LAYER DETAILS:
// Learnable soft assignment to K cluster centers
FUNCTION NetVLAD(features):  // features: H√óW√óD
    // Soft assignment: compute assignment weights to K clusters
    FOR each spatial location (i,j):
        FOR each cluster k:
            // Softmax over clusters
            a_k(i,j) = exp(w_k^T * f(i,j) + b_k) / Œ£_k' exp(w_k'^T * f(i,j) + b_k')
    
    // Aggregate residuals
    FOR each cluster k:
        FOR each dimension d:
            V_k,d = Œ£_(i,j) a_k(i,j) * (f_d(i,j) - c_k,d)
            // c_k,d is the d-th dimension of cluster center k
    
    // Intra-normalization
    FOR each cluster k:
        V_k = L2_normalize(V_k)
    
    RETURN V  // Shape: K√óD

COMPUTATIONAL REQUIREMENTS:
- CNN backbone (VGG16): ~15 GFLOPs per image
- NetVLAD layer: ~0.5 GFLOPs
- Descriptor size: 4096-32768 dimensions (16-128 KB per image)
- Database search: O(N) similarity computations, ~1-5 ms for N=1000 images
- Mobile optimization: Use MobileNet backbone ‚Üí ~0.5 GFLOPs total
- Inference time: 50-100 ms on mobile GPU
                    </pre>
                </div>

                <h4>Performance and Trade-offs</h4>
                <ul>
                    <li><strong>Accuracy:</strong> 85-95% correct localization within 5 meters on indoor datasets</li>
                    <li><strong>Robustness:</strong> Handles viewpoint changes up to 45¬∞, moderate lighting variations</li>
                    <li><strong>Scalability:</strong> Efficient for databases of 1,000-10,000 locations; larger databases require approximate nearest neighbor search (e.g., FAISS)</li>
                    <li><strong>Mobile deployment:</strong> MobileNet-NetVLAD achieves 10-20 FPS with acceptable accuracy degradation</li>
                </ul>

                <h4>Application to Blind Navigation</h4>
                <p>
                    VPR enables several critical navigation capabilities:
                </p>
                <ul>
                    <li><strong>"You are near the main lobby":</strong> Recognizing high-level location context</li>
                    <li><strong>"This is the hallway to Room 304":</strong> Confirming the user is on the correct route</li>
                    <li><strong>"You've returned to the elevator":</strong> Loop closure for multi-floor navigation</li>
                    <li><strong>Automatic map building:</strong> Constructing spatial maps during initial exploration for later reuse</li>
                </ul>

                <div class="figure">
                    <img src="images/vpr-matching.jpg" alt="Visual place recognition showing query image matched to database image" style="max-width: 700px;">
                    <p class="figure-caption">Figure 6: Visual Place Recognition example. Query image (left) is matched to database image (right) despite different viewpoint and lighting. Similarity score: 0.87. Source: <a href="bibliography.html#ref5" class="citation">[5]</a></p>
                </div>
            </div>

            <div class="landmark-graph">
                <h3>Landmark Graph Construction: Spatial Topology for Navigation</h3>
                
                <h4>Motivation: From Detection to Navigation</h4>
                <p>
                    Individual landmark detections provide local spatial information, but navigation requires understanding the <strong>topological relationships</strong> between landmarks: which doors connect to which hallways, how elevators relate to floor layouts, and what sequence of landmarks leads to a destination. A <strong>landmark graph</strong> encodes this spatial topology as a navigable structure.
                </p>

                <h4>Graph Representation</h4>
                <p>
                    A landmark graph G = (V, E) consists of:
                </p>
                <ul>
                    <li><strong>Vertices (V):</strong> Detected landmarks (doors, elevators, intersections) with associated properties:
                        <ul>
                            <li>Landmark type (door, elevator, stairs, etc.)</li>
                            <li>Semantic label (e.g., "Room 304", "Main Elevator")</li>
                            <li>3D position estimate (from VPR or SLAM)</li>
                            <li>Visual descriptor (for recognition)</li>
                        </ul>
                    </li>
                    <li><strong>Edges (E):</strong> Navigable connections between landmarks with properties:
                        <ul>
                            <li>Distance (estimated from odometry or depth integration)</li>
                            <li>Direction (relative heading change)</li>
                            <li>Traversal type (walk straight, turn left/right, take elevator)</li>
                        </ul>
                    </li>
                </ul>

                <h4>Graph Construction Process</h4>
                <p>
                    The landmark graph is built incrementally as the user explores the environment:
                </p>
                <ol>
                    <li><strong>Landmark detection:</strong> Continuously detect landmarks using object detection and OCR</li>
                    <li><strong>Landmark tracking:</strong> Track landmarks across frames using visual features and depth to maintain identity</li>
                    <li><strong>Vertex creation:</strong> When a new landmark is first observed, create a graph vertex with its properties</li>
                    <li><strong>Odometry integration:</strong> Track user motion between landmarks using visual-inertial odometry (ARCore/ARKit)</li>
                    <li><strong>Edge creation:</strong> When transitioning from one landmark to another, create an edge encoding the motion (distance, direction)</li>
                    <li><strong>Loop closure:</strong> When VPR recognizes a previously visited landmark, create an edge connecting the current position to the recognized vertex, forming a loop</li>
                    <li><strong>Graph optimization:</strong> Adjust vertex positions and edge lengths to minimize inconsistencies (e.g., using pose graph optimization)</li>
                </ol>

                <h4>Lightweight Implementation for Mobile</h4>
                <p>
                    Full SLAM (Simultaneous Localization and Mapping) systems are computationally expensive. For mobile deployment, a simplified approach suffices:
                </p>
                <ul>
                    <li><strong>Sparse graph:</strong> Only major landmarks (doors, elevators, intersections) become vertices, not every visual feature</li>
                    <li><strong>Topological representation:</strong> Focus on connectivity rather than precise metric positions</li>
                    <li><strong>Incremental construction:</strong> Build graph during initial exploration; reuse for subsequent visits</li>
                    <li><strong>Cloud storage:</strong> Store building-specific graphs in the cloud; download when entering a known building</li>
                </ul>

                <h4>Turn-by-Turn Guidance Generation</h4>
                <p>
                    Once a landmark graph is constructed, navigation from current location to destination becomes a graph search problem:
                </p>
                <ol>
                    <li><strong>Localization:</strong> Use VPR to determine current vertex in the graph</li>
                    <li><strong>Destination specification:</strong> User specifies destination (e.g., "Room 304" via voice input)</li>
                    <li><strong>Path planning:</strong> Apply Dijkstra's or A* algorithm to find shortest path from current vertex to destination vertex</li>
                    <li><strong>Instruction generation:</strong> Convert path edges into natural language instructions:
                        <ul>
                            <li>"Walk forward 15 meters"</li>
                            <li>"Turn right at the next intersection"</li>
                            <li>"Take the elevator to floor 3"</li>
                            <li>"Room 304 is the second door on your left"</li>
                        </ul>
                    </li>
                    <li><strong>Real-time guidance:</strong> As user moves, track progress along the path and provide timely instructions</li>
                </ol>

                <h4>Example: NavCog System</h4>
                <p>
                    The NavCog system developed at Carnegie Mellon University <a href="bibliography.html#ref1" class="citation">[1]</a> exemplifies landmark graph-based navigation:
                </p>
                <ul>
                    <li><strong>Graph construction:</strong> Building administrators create landmark graphs using a mapping tool</li>
                    <li><strong>Localization:</strong> Combines BLE beacons with visual landmark detection for robust positioning</li>
                    <li><strong>Navigation:</strong> Provides turn-by-turn audio instructions based on graph path planning</li>
                    <li><strong>User feedback:</strong> Blind users reported successful navigation in complex university buildings</li>
                </ul>

                <div class="figure">
                    <img src="images/landmark-graph.jpg" alt="Visualization of landmark graph showing vertices and edges" style="max-width: 700px;">
                    <p class="figure-caption">Figure 7: Landmark graph visualization for an office floor. Vertices (circles) represent doors and elevators; edges (lines) represent navigable paths. Shortest path from current location (green) to destination (red) is highlighted. Source: <a href="bibliography.html#ref1" class="citation">[1]</a></p>
                </div>
            </div>

            <div class="context-awareness">
                <h3>Context-Aware Navigation: Adaptive Guidance</h3>
                
                <h4>Beyond Static Maps: Dynamic Context</h4>
                <p>
                    Effective navigation systems adapt guidance based on contextual factors:
                </p>
                <ul>
                    <li><strong>User state:</strong> Walking speed, confidence level, familiarity with environment</li>
                    <li><strong>Environmental conditions:</strong> Crowd density, noise level, lighting</li>
                    <li><strong>Task urgency:</strong> Routine navigation vs. emergency egress</li>
                    <li><strong>Landmark availability:</strong> Adapt route if expected landmarks are occluded or missing</li>
                </ul>

                <h4>Adaptive Instruction Timing</h4>
                <p>
                    Context-aware systems adjust when and how instructions are delivered:
                </p>
                <ul>
                    <li><strong>Anticipatory guidance:</strong> "In 10 meters, turn right" gives users time to prepare</li>
                    <li><strong>Confirmation feedback:</strong> "Correct, you've turned right" confirms successful action</li>
                    <li><strong>Error recovery:</strong> "You've passed the turn; turn around" when user misses an instruction</li>
                    <li><strong>Verbosity adaptation:</strong> Detailed instructions for unfamiliar areas, concise for familiar routes</li>
                </ul>

                <h4>Multi-Modal Feedback</h4>
                <p>
                    Combining audio and haptic feedback improves guidance effectiveness:
                </p>
                <ul>
                    <li><strong>Audio:</strong> Verbal instructions and landmark descriptions</li>
                    <li><strong>Haptic:</strong> Vibration patterns indicating direction (e.g., left/right vibration for turns)</li>
                    <li><strong>Spatial audio:</strong> 3D audio cues indicating landmark direction</li>
                </ul>

                <p>
                    This multi-modal approach reduces cognitive load and enables navigation in noisy environments where audio alone may be insufficient.
                </p>
            </div>

            <div class="integration-summary">
                <h3>Integration: Complete Navigation Pipeline</h3>
                <p>
                    A complete indoor navigation system integrates all components discussed:
                </p>
                <ol>
                    <li><strong>Continuous landmark detection:</strong> Object detection, OCR, and keypoint detection identify nearby landmarks</li>
                    <li><strong>Visual place recognition:</strong> Recognizes current location within known building</li>
                    <li><strong>Graph localization:</strong> Determines current vertex in landmark graph</li>
                    <li><strong>Path planning:</strong> Computes route to user-specified destination</li>
                    <li><strong>Context-aware guidance:</strong> Delivers adaptive, multi-modal navigation instructions</li>
                    <li><strong>Continuous tracking:</strong> Monitors progress and updates guidance in real-time</li>
                </ol>
                <p>
                    This integrated approach transforms raw sensor data into actionable navigation guidance, enabling blind users to navigate complex indoor environments independently and safely.
                </p>
            </div>
        </section>

        <div class="navigation-footer">
            <a href="techniques.html" class="prev-page">‚Üê Previous: Detection Techniques</a>
            <a href="review.html" class="next-page">Next: Successes & Failures ‚Üí</a>
        </div>
    </main>

    <footer class="site-footer">
        <p>&copy; 2025 Indoor Navigation Tutorial | Graduate-Level Research Review</p>
    </footer>
</body>
</html>

